\subsection{Modeling Modalities and the Nature of Artificial Life}

The roots of scientific understanding lie in juxtaposition of abstract models with the natural world \citep{banzhaf2016defining}.
Modality and structure of these models vary tremendously, but all relate axiomatic formulations to consequent implications.
Verbal models relate abstractions through qualitative logic.
Mathematical models interface quantified entities through geometric, algebraic, analytic, combinatorial, and other formal devices.
Crucially, symbolic representations allow mathematical approaches to resolve sophisticated relationships through compact, closed-form descriptions.
Finally, physical models extend back into the natural world, constructing tangible analogs, generally as footholds for exploration of verbal and mathematical models (i.e., down-scaled or stripped down).

Digital electronic processing ushered in computational models, largely unrealized up to that point.
Such models reify vast layers --- today, trillions --- of exactly prescribed interactions.
This capability grants entirely new avenues to study collective behavior of complex systems.
Although alike in telling outcomes consequent to axiomatic suppositions, the scope of some computational models, however, can erode their descriptiveness in elucidating intermediate mechanistic steps.
Akin to physical models, in some cases it becomes necessary to resort to further cycles of experimentation and model-building analogous to those applied against the natural world.
Artificial life, largely coming to fruition at the advent of modern computing, rests at this juncture.
The field employs models, physical and computational, as an experimental subject to scaffold understanding through verbal and mathematical models \citep{bedau2003artificial}.

Today, the divide between physical and computational modeling modalities is generally hard-cut.
Unlike physical models, computational outcomes are perfectly attributable: mathematical specification wholly determines every state change.
Although exact in tracing out abstraction, computation occurs through physical processes --- in the case of digital electronic computing, via voltage and current in physical hardware.
Borrowing terminology typically applied to situating of cognition \citep{etxeberria1998embodiment}, computation in practice is physically embodied.
Usually, this fact holds relevant only insofar as performance considerations structure particulars in program design.
Recently, however, concern has grown over mounting costs of preserving perfect rectitude across ever more expansive contingents of hardware.

In this piece, we discuss alternative ``best-effort'' strategies, starting first with a critical examination of their relationship to artificial life ethos and existing research agendas within the field.
We then propose a pragmatic approach to make better use of existing high-performance computing (HPC) hardware, with promising initial results in exploratory benchmarks.

\subsection{Best-effort Computing}

As HPC scales, it becomes increasingly difficult to write software that makes efficient use of available hardware and, simultaneously, provides reproducible results (or even near-perfectly reproducible results --- i.e., up to effects from floating point non-transitivity).
Difficulty largely boils down to two quagmires:
\begin{enumerate}
\item synchronicity paces to the slowest laggard, which tends towards increasing extremity as pool size grows; and
\item incidence of rare compound failures rises with component count, necessitating greater redundancies to prevent fatal error.
\end{enumerate}
Although subject of many fruitful mitigation efforts, costs nonetheless mount in performance overhead and engineering effort.
More insidious, still, are soft errors --- faults, such as lost messages or corrupted memory, that transpire silently \citep{karnik2004characterization}.
Further miniaturization and voltage reduction, which are assumed as a likely vehicle for continuing advances in hardware efficiency and performance, could conceivably worsen susceptibility to such errors \citep{dongarra2014applied,kajmakovic2020challenges}.
Vanishingly rare at the scale of single-processor computing and generally mitigable through redundancy (e.g., duplicate execution or error correction codes \citep{vankeirsbilck2015soft,sridharan2015memory}), countermeasures must ramp up with scale as occurrence accelerates to non-trivial rates \citep{sridharan2015memory,scoles2018cosmic}.

The worsening marginal costs of synchronization, fault recovery, and error correction beg the question whether it is viable to maintain, or even strive to maintain, the reliable digital machine model at scale \citep{dongarra2014applied}.
Indeed, software and hardware that concede some incorrectness or indeterminism --- the ``best-effort model'' --- have been shown to improve speed \citep{chakrapani2008probabilistic}, energy efficiency \citep{chakrapani2008probabilistic,bocquet2018memory}, and scalability \citep{meng2009best}.
Discussion around ``approximate computing'' overlaps significantly with ``best-effort computing,'' although focusing more heavily on using algorithm design to sidestep non-essential computation (i.e., reduced floating point precision, inexact memoization, etc.) \citep{mittal2016survey}.
However, best-effort computing stands distinct in breach of execution's adherenece to idealization.
The intrusion of extraneous influence into programmed execution relaxes best-effort simulation into a sort of liminally physical model.
In this light, the best-effort proposition appears starkly exotic.
In an alternate framing, however, this step simply represents further pragmatic compromise with the physical reality of computational hardware --- with the difference of kind manifested as spillover from constraining aspects of program specification to eroding the specifiable purview.

The suitability of best-effort approaches varies from application to application.
Real-time control systems that cannot afford to pause or retry, by necessity, fall into the best-effort category \citep{rahmati2011computing, rhodes2020real}.
Other domains are clear cut against departure from the reliable digital machine model --- for example, due to regulatory issues \citep{dongarra2014applied}.
However, a subset of HPC applications can tolerate occasional flaws, or even fundamentally nondeterminism, in computation \citep{chakradhar2010best}.
Optimization domains, typically having no singular ``correct'' outcome, make an obvious fit;
those, especially, leveraging heuristics or pseudo-stochastic methods that tend to exploit noise rather than destabilize due to it \citep{chakrapani2008probabilistic,chakradhar2010best}.
In this vein, best-effort applications to stochastic gradient descent for artificial neural network applications have proven fruitful \citep{dean2012large,zhao2019elastic,niu2011hogwild,noel2014dogwild,rhodes2020real}.
Even within directly representational simulation, however, notable success has stemmed from machine learning approximations of simulation mechanics \citep{behler2007generalizedkochkov2021machine}.
(Noted, such work categorizes as ``approximate'' rather than ``best-effort.'')

The pliability of artificial life behooves best-effort strategy.
As artificial life already brings together purely-computational, explicitly-physical (i.e., robotics) and purely-physical (i.e., biochemical) modeling modalities, the blurred nature of best-effort computational models make a natural fit.
Indeed, best-effort, real-time computing approaches have already made several notable appearances in artificial life work.
Tom Ray's Network Tierra pushed cyber-physical model hybridization to the logical extreme.
This project instantiated populations across an at-will federation of distributed simulation endpoints, with individuals migrating across peer-to-peer internet connections.
Exposure of simulation dynamics to external factors was, in fact, an explicit aim rather than an engineering compromise;
influence of human-driven fluctuations in internet traffic and server load due to seasonality, day/night cycles, etc. on evolved behaviors was highlighted of key interest \citep{ray1995proposal}.

Most notable in systematically advocating, developing, and realizing best-effort simulation, though, is David Ackley's work on indefinite scalability in the context of open-ended evolution.
This line of work paves a trajectory toward clustering modular distributed hardware at a theoretically unlimited scale.
Best-effort asynchronicity and error-tolerance come as necessary preconditions to this end, as does decentrality and spatiality \citep{ackley2011pursue}.
Notable accomplishments in this vein include the Movable Feast Machine (MFM) execution framework, designed as a substrate for distributed spatial automata \citep{ackley2013movable}; the ULAM programming language, designed for expressive MFM configurations \citep{ackley2016ulam}; and, more recently, impressive emergent structures and replicators engineered within within these substrates \citep{ackley2018digital,ackley2023robust}.
Ackley's work is specially notable in its extension to custom hardware demonstrative of indefinite scalability principles, including Illuminata X Machina \citep{ackley2011homeostatic} and, presently, the T2 Tile \citep{ackley2023robust}.
Perhaps the most emphasized thread through Ackley's oeuvre, though, is a broad wholehearted boosterism for the significance of substrate capacity to artificial life's core ambitions \citep{ackley2014indefinitely}.
Suggestions to this end appear elsewhere \citep{channon2019maximum,banzhaf2016defining,moreno2019toward}, as well, and we echo emphasis on system scale-up here.

\subsection{Scale in Context}

A reference point is instructive to discussing scope in artificial life simulation.
Take as an example the Avida digital evolution platform, a popular software system for evolutionary experiments with self-replicating computer programs.
In this system, a population of ten thousand digital organisms can undergo approximately twenty thousand generations per day \citep{ofria2009artificial}.
This equates to about two hundred million individual replication cycles.
Each flask in the Lenski Long-Term Evolution Experiment hosts a similar number of replication cycles; with an effective population size of 30 million \textit{E. coli} that undergo a bit more than 6.6 doublings per day, the bacteria experience about 180 million replication events per day \citep{good2017dynamics}.
As another point of commparison, in Ratcliff’s work studying the evolution of multicellularity in \textit{S. cerevisiae}, about six doublings per day occur among a population numbering on the order of a billion cells \citep{ratcliff2012experimental}.

Although colloquialised as ``worlds,'' with serial processing power the scope of typical artificial life simulation aligns, in naive terms, near a laboratory flask.
Of course, such a comparison neglects profound incommensurabilities between Avidians and bacteria or yeast.
Natural organisms have vastly more genetic content and phenotypic state, as well as more and more diverse interactions with the environment and with other cells.

Since the field's inception, artificial life has ridden a steady current of microprocessor innovation.
As improvements in serial processing dried up around the turn of the century \citep{sutter2005free}, parallel and distributed became ascendant.
As a result, it has become routine to dispatch independent instantiations of simulation runs across hardware units.
In scientific contexts, this practice yields replicate datasets that provide statistical power to answer research questions \citep{dolson2017spatial}.
In applied contexts, this practice yields many converged populations that can be scavenged for the best solutions overall \citep{hornby2006automated}.
Another established practice is to use ``island models'' where individuals are transplanted between populations residing across distributed hardware \citep{gorges1990explicit}.
Notably, asynchronous approaches are common and effective in these models \citep{abdelhafez2019performance}.

Koza and collaborators’ genetic programming work with a 1,000-CPU Beowulf cluster typifies the island model approach \citep{bennett1999building}.
In recent years, Sentient Technologies spearheaded evolutionary computation projects on an unprecedented computational scale, comprising over a million CPUs and capable of a peak performance of 9 petaflops \citep{miikkulainen2019evolving}.
According to its proponents, the scale and scalability of this ``DarkCycle'' system was a key aspect of its conceptualization \citep{gilbert2015artificial}.
Much of the assembled infrastructure was pieced together from heterogeneous providers and employed on a time-available basis \citep{blondeau2009distributed}.
Unlike typical island models where selection occurs entirely independently on each CPU, this scheme transferred evaluation criteria between computational instances in addition to individual genomes \citep{hodjat2013distributed}.
Sentient Technologies also notably exploited a large pool of hardware accelerators (e.g., 100 GPUs) in work evolving neural network architectures by performing each candidate architecture's costly model training and evaluation process \citep{miikkulainen2019evolving}.

Existing parallel and distributed digital evolution systems typically minimize interaction between simulation components on disjoint hardware.
Such independence facilitates simple and efficient implementation.
This approach typically involves independent evaluation of sub-populations (i.e., island models) or individuals (i.e., primary-subordinate or controller-responder parallelism \citep{cantu2001master}).
Cases where evaluation of a single individual are parallelized often involve data-parallel evaluation over a set of independent test cases, which are subsequently consolidated into a single fitness profile \citep{harding2007fast_springer, langdon2019continuous}.

However, several notable parallel and distributed digital evolution systems have incorporated rich interactions between parallelized simulation components.
Harding applied GPU acceleration to cellular automata models of artificial development systems, which involve intensive interaction between spatially-distributed instantiation of a genetic program \citep{harding2007fast_ieee}.
Likewise, Network Tierra featured arbitrary communication between digital organisms residing on different machines \citep{ray1995proposal}.
More recently, in a continuation of much earlier work, Christian Heinemann's ongoing ALIEN project has leveraged GPU acceleration for perform physics-based simulation of soft body agents within a 2D arena \citep{heinemann2008artificial}.

\section{Paths Forward}

Implicit among much of the field, it seems, is an anticipation that order-of-magnitude changes in artificial life systems may unlock a qualitative sea change in simulation outcomes.
This is not an entirely unreasonable notion of a possible future of artificial life.

Spectacular advances achieved with artificial neural networks over the last decade illuminate one conceivable progression of events.
As with digital evolution, artificial neural networks (ANNs) were traditionally understood as a versatile but auxiliary methodology --- both techniques have been described as ``the second best way to do almost anything'' \citep{miaoulis2008intelligent,eiben2015introduction}.
However, the utility and ubiquity of ANNs has since increased dramatically \citep{marcus2018deep}.
The development of AlexNet is widely considered pivotal to this transformation.
AlexNet united methodological innovations from the field (such as big datasets, dropout, and ReLU) with GPU computing that enabled training of orders-of-magnitude-larger networks.
In fact, some aspects of their deep learning architecture were expressly modified to accommodate multi-GPU training \citep{krizhevsky2012imagenet}.
By adapting existing methodology to exploit commercially available hardware, AlexNet spurred the greater availability of compute resources to the research domain and eventually the introduction of custom hardware to expressly support deep learning \citep{jouppi2017datacenter}.

Likewise, progress of artificial life systems along a path, in the most ambitious framing, to indefinite scalability seems likely to unfold through incremental investments spurred by progressive scientific achievements.
To that end, observability \citep{moreno2023toward} and interpretability \citep{horgan1995complexity} will be key concerns, as will intentionally focuusing engineering effort to support hypothesis-driven objectives.
In tandem with more farsighted efforts, progress will also require prosaic leverage of existing, commercially-available parallel and distributed compute resources at circumstantially-feasible scales.

Such work should be made with an eye for contribution back to HPC.
As noted earlier, the unique character of artificial life simulation suits it to serve at the tip of the spear in HPC evolution.
High-performance computing hardware with transformative capabilities is coming to market right now, and some of it --- like the Cerebras wafer scale engine \citep{lauterbach2021path} --- is built explicitly for decentralized, asynchronous computation.
Given ubiquity of deep net training and stencil-based numerical solvers in applications (CITE), programming for agent-based simulation can be of interest insofar as it flexes harware capabilities in unimagined ways.
Effort to establish artificial life simulation as a flagship HPC application could be of mutual benefit.

\subsection{On-hardware Experiments}

Here, we present a set of on-hardware experiments to empirically characterize Conduit's best-effort communication model.
In order to survey across workload profiles, we tested performance under both a communication-intensive graph coloring solver and a compute-intensive artificial life simulation.

First, we determine whether best-effort communication strategies can benefit performance compared to the traditional perfect communication model.
We considered two measures of performance: computational steps executed per unit time and solution quality achieved within a fixed-duration run window.

We compare the best-effort and perfect-computation strategies across processor counts, expecting to see the marginal benefit from best-effort communication increase at higher processor counts.
We focus on weak scaling, growing overall problem size proportional to processor count.
Put another way, we hold problem size per processor constant.%
\footnote{
As opposed to strong scaling, where the problem size is held fixed while processor count increases.
}
This approach prevents interference from shifts in processes' workload profiles in observation of the effects of scaling up processor count.

To survey across hardware configurations, we tested scaling CPU count via threading on a single node and scaling CPU count via multiprocessing with each process assigned to a distinct node.
In addition to a fully best-effort mode and a perfect communication mode, we also tested two intermediate, partially synchronized modes: one where the processor pool completed a global barrier (i.e., they aligned at a synchronization point) at predetermined, rigidly scheduled time points and another where global barriers occurred on a rolling basis spaced out by fixed-length delays from the end of the last synchronization.%
\footnote{
Our motivation for these intermediate synchronization modes was interest in the effect of clearing any potentially-unbounded accumulation of message backlogs on laggard processes.
}

Second, we sought to more closely characterize variability in message dispatch, transmission, and delivery under the best-effort model.
Unlike perfect communication, real-time volatility affects the outcome of computation under the best-effort model.
Because real-time processing speed degradations and message latency or loss alters inputs to simulation elements, characterizing the distribution of these phenomena across processing components and over time is critical to understanding the actual computation being performed.
For example, consistently faster execution or lower messaging latency for some subset of processing elements could violate uniformity or symmetry assumptions within a simulation.
It is even possible to imagine reciprocal interactions between real-time best-effort dynamics and simulation state.
In the case of a positive feedback loop, the magnitude of effects might become extreme.
For example, in artificial life scenarios, agents may evolve strategies that selectively increase messaging traffic so as to encumber neighboring processing elements or even cause important messages to be dropped.

We monitor five aspects of real-time behavior, which we refer to as quality of service metrics \citep{karakus2017quality},
\begin{itemize}
  \item wall-time simulation update rate (``simstep period''),
  \item simulation-time message latency,
  \item wall-time message latency,
  \item steadiness of message inflow (``delivery bunching''), and
  \item delivery failure rate.
\end{itemize}

In an initial set of experiments, we use the graph coloring problem to test this suite of quality of service metrics across runtime conditions expected to strongly influence them.
We compare
\begin{itemize}
  \item increasing compute workload per simulation update step,
  \item within-node versus between-node process placement, and
  \item multithreading versus multiprocessing.
\end{itemize}
We perform these experiments using a graph coloring solver configured to maximize communication relative to computation (i.e., just one simulation element per CPU) in order to maximize sensitivity of quality of service to the runtime manipulations.

Finally, we extend our understanding of performance scaling from the preceding experiments by analyzing how each quality of service metric fares as problem size and processor count grow together, a ``weak scaling'' experiment.
This analysis would detect a scenario where raw performance remains stable under weak scaling, but quality of service (and, therefore, potentially quality of computation) degrades.

Experiments employ a new software framework, the Conduit C++ Library for Best-Effort High Performance Computing \citep{moreno2021conduit}.
The Conduit library provides tools to perform best-effort communication in a flexible, intuitive interface and uniform inter-operation of serial, parallel, and distributed modalities.
Although Conduit currently implements distributed functionality via MPI intrinsics, future work will explore lower-level protocols like InfiniBand Unreliable Datagrams \citep{kashyap2006ip, koop2007high}.
