\input{lib/dissertationonly.tex}

\section{Introduction}
\label{sec:introoduction;ch:conduit}

The parallel and distributed processing capacity of high-performance computing (HPC) clusters continues to grow rapidly and enable profound scientific and industrial innovations \citep{gagliardi2019international}.
These advances in hardware capacity and economy afford great opportunity, but also pose a serious challenge: developing approaches to effectively harness it.
As HPC systems scale, it becomes increasingly difficult to write software that makes efficient use of available hardware and also provides reproducible results (or even near-perfectly reproducible results --- i.e., up to effects from floating point non-transitivity) consistent with models of computation as being performed a reliable digital machine \citep{heroux2014toward}.

The bulk synchronous parallel (BSP) model, which is prevalent among HPC applications \citep{dongarra2014applied}, illustrates the challenge.
This model segments fragments of computation into sequential global supersteps, with fragments at superstep $i$ depending only on data from strictly preceding fragments $<i$, often just $i-1$.
Computational fragments are assigned across a pool of available processing components.
The BSP model assumes perfectly reliable messaging: all dispatched messages between computational fragments are faithfully delivered.
In practice, realizing this assumption introduces overhead costs: secondary acknowledgment messages to confirm delivery and mechanisms to dispatch potential resends as the need arises.
Global synchronization occurs between supersteps, with computational fragments held until their preceding superstep has completed \citep{valiant1990bridging}.
This ensures that computational fragments will have at hand every single expected input, including those required from fragments located on other processing elements, before proceeding.
So, supersteps only turn over once the entire pool of processing components have completed their work for that superstep.
Put another way, all processing components stall until the most laggardly component catches up.
In a game of double dutch with several jumpers, this would be like slowing the tempo to whoever is most slow-footed each particular turn of the rope.

Heterogeneous computational fragments, with some easy to process and others much slower, would result in poor efficiency under a naive approach where each processing element handled just one fragment.
Some processing elements with easy tasks would finish early then idle while more difficult tasks carry on.
To counteract such load imbalances, programmers can allow for ``parallel slack'' by ensuring computational fragments greatly outnumber processing elements or even performing dynamic load balancing at runtime \citep{valiant1990bridging}.

Unfortunately, hardware factors on the underlying processing elements ensure that inherent global superstep jitter will persist: memory access time varies due to cache effects, message delivery time varies due to network conditions, extra processing due to error detection and recovery, delays due to unfavorable process scheduling by the operating system, etc. \citep{dongarra2014applied}.
Power management concerns on future machines will likely introduce even more variability \citep{gropp2013programming}.
Worse yet, as we work with more and more processes, the expected magnitude of the worst-sampled jitter grows and grows --- and in lockstep with it, our expected superstep duration.
In the double dutch analogy, with enough jumpers, at almost every turn of the rope someone will need to stop and tie their shoe.
The global synchronization operations underpinning the BSP model further hinder its scalability.
Irrespective of time to complete computational fragments within a superstep, the cost of performing a global synchronization operation increases with processor count \citep{dongarra2014applied}.

Efforts to recover scalability by relaxing superstep synchronization fall under two banners.
The first approach, termed ``Relaxed Bulk-Synchronous Programming'' (rBSP), hides latency by performing collective operations asynchronously, essentially allowing useful computation to be performed at the same time as synchronization primitives for a single superstep percolate through the collective \citep{heroux2014toward}.
So, the time cost required to perform that synchronization can be discounted, up to the time taken up by computational work at one superstep.
Likewise, individual processes experiencing heavier workloads or performance degradation due to hardware factors can fall behind by up to a single superstep without slowing the entire collective.
However, this approach cannot mask synchronization costs or cumulative performance degradation exceeding a single superstep's duration.
The second approach, termed relaxed barrier synchronization, forgoes global synchronization entirely \citep{kim1998relaxed}.
Instead, computational fragments at superstep $i$ only wait on expected inputs from the subset of superstep $i-1$ fragments that they directly interact with.
Imagine a double-dutch routine where each jumper exchanges patty cakes with both neighboring jumpers at every turn of the rope.
Relaxed barrier synchronization would dispense entirely with the rope.
Instead, players would be free to proceed to their next round of patty cakes as soon as they had successfully patty-caked both neighbors.
With $n$ players, player 0 could conceivably advance $n$ rounds ahead of player $n-1$ (each player would be one round ahead of their right neighbor).
Assuming fragment interactions form a graph structure that persists across supersteps, in the general case before causing the entire collective to slow an individual fragment can fall behind at most a number of supersteps equal to the graph diameter \citep{gamell2015local}.
Even though this approach can shield the collective from most one-off performance degradations of a single fragment (especially in large-diameter cases), persistently laggard hardware or extreme one-off degradations will ultimately still hobble efficiency.
Dynamic task scheduling and migration aim to address this shortcoming, redistributing work in order to ``catch up'' delinquent fragments \citep{acun2014parallel}.
With our double-dutch analogy, we could think of this something like a team coach temporarily benching a jumper who skinned their knee and instructing the other jumpers to pick up their roles in the routine.

In addition to concerns over efficiency, resiliency poses another inexorable problem to massive HPC systems.
In small scales, it can suffice to assume that failures occur negligibly, with any that do transpire likely to cause an (acceptably rare) global interruption or failure.
At large scales, however, software crashes and hardware failures become the rule rather than the exception \citep{dongarra2014applied} --- running a simulation to completion could even require so many retries as to be practically infeasible.
A typical contemporary approach to improve resiliency is checkpointing: the system periodically records global state then, when a failure arises, progress is rolled back to the most recent global known-good state and runtime restarts \citep{hursey2007design}.
Global checkpoint-based recovery is expensive, especially at scale due to overhead associated with regularly recording global state, losing progress since the most recent checkpoint, and actually performing a global teardown and restart procedure.
In fact, at large enough scales global recovery durations could conceivably exceed mean time between failures, making any forward simulation progress all but impossible \citep{dongarra2014applied}.
The local failure, local recovery (LFLR) paradigm eschews global recovery by maintaining persistent state on a process-wise basis and providing a recovery function to initialize a step-in replacement process \citep{heroux2014toward,teranishi2014toward}.
In practice, such an approach can require keeping running logs of all messaging traffic in order to replay them for the benefit of any potential step-in replacement \citep{chakravorty2004fault}.
Returning once more to the double dutch analogy, LFLR would transpire as something like a handful teammates pulling a stricken teammate aside to catch them up after an amnesia attack (rather than starting the entire team's routine back at the top of the current track).
The intervening jumpers would have to remind the stricken teammate of a previously recorded position then discreetly re-feign some of their moves that the stricken teammate had cued off of between that recorded position and the amnesia episode.

The possibility of multiple simultaneous failure (perhaps, for example, of dozens of processes resident on a single node) poses an even more difficult, although not insurmountable, challenge for LFLR that would likely necessitate even greater overhead.
On approach involves pairing up with a remote ``buddy'' process.
The ``buddy'' hangs to the focal process' snapshots and is carbon-copied on all of that process' messages in order to ensure an independently survivable log.
Unfortunately, this could potentially require forwarding all messaging traffic between simulation elements coresident on the focal process to its buddy, dragging inter-node communication into some otherwise trivial simulation operations \citep{chakravorty2007fault}.
Efforts to ensure resiliency beyond single-node failures currently appear unnecessary \citep[p. 12]{ni2016mitigation}.
Even though LFLR saves the cost of global spin-down and spin-up, all processes will potentially have to wait for work lost since the last checkpoint to be recompleted, although in some cases this could be helped along by tapping idle hardware to take over delinquent work from the failed process and help catch it up \citep{dongarra2014applied}.

Still more insidious to the reliable digital machine model, though, are soft errors --- events where corruption of data in memory occurs, usually do to environmental interference (i.e., ``cosmic rays'') \citep{karnik2004characterization}.
Further miniaturization and voltage reduction, which are assumed as a likely vehicle for continuing advances in hardware efficiency and performance, could conceivably worsen susceptibility to such errors \citep{dongarra2014applied,kajmakovic2020challenges}.
What makes soft errors so dangerous is their potential indetectability.
Unlike typical hardware or software failures, which result in an explicit, observable outcome (i.e., an error code, an exception, or even just a crash), soft errors can transpire silently and lead to incorrect computational results without leaving anyone the wiser.
Luckily, soft errors occur rarely enough to be largely neglected in most single-processor applications (except the most safety-critical settings); however, at scale soft errors occur at a non-trivial rate \citep{sridharan2015memory,scoles2018cosmic}.
Redundancy (be it duplicated hardware components or error correction codes) can reduce the rate of uncorrected (or at least undetected) soft errors, although at a non-trivial cost \citep{vankeirsbilck2015soft,sridharan2015memory}.
In some application domains with symmetries or conservation principles, the rate of soft errors (or, at least, silent soft errors) could be also reduced through so-called ``skeptical'' assertions at runtime \citep{dongarra2014applied}, although this too comes at a cost.

Even if soft errors can be effectively eradicated --- or at least suppressed to a point of inconsequentiality --- the nondeterministic mechanics of fault recovery and dynamic task scheduling could conceivably make guaranteeing bitwise reproducibility at exascale effectively impossible, or at least an unreasonable engineering choice \citep{dongarra2014applied}.
However, the assumption of the reliable digital machine model remains near-universal within parallel and distributed algorithm design \citep{chakradhar2010best}.
Be it just costly or simply a practical impossibility, the worsening burden of synchronization, fault recovery, and error correction begs the question of whether it is viable to maintain, or even to strive to maintain, the reliable digital machine model at scale.
Indeed, software and hardware that relaxes guarantees of correctness and determinism --- a so-called ``best-effort model'' --- have been shown to improve speed \citep{chakrapani2008probabilistic}, energy efficiency \citep{chakrapani2008probabilistic,bocquet2018memory}, and scalability \citep{meng2009best}.
Discussion around ``approximate computing'' overlaps significantly with ``best-effort computing,'' although focusing more heavily on using algorithm design to shirk non-essential computation (i.e., reducing floating point precision, inexact memoization, etc.) \citep{mittal2016survey}.
As technology advances, computing is becoming more distributed and we are colliding with physical limits for speed and reliability.
%Theoretical exploration of constraints distributed systems will face at the asymptote of technological (and even physical) constraints, performed under the banner of ``indefinite scalability'' in \citep{ackley2011pursue}, highlights an essential role --- at large enough scales, essentially a design inevitability --- for best-effort methods.
%Specifically, this theory finds as necessary asynchronous operation and graceful degradation under hardware failure (in addition to decentralized networking and interchangeable hardware components).
Massively distributed systems are becoming inevitable, and indeed if we are to truly achieve ``indefinite scalability'' \citep{ackley2011pursue} we must shift from guaranteed accuracy to best-effort methods that operate asynchronously and degrade gracefully under hardware failure.

The suitability of the best-effort model varies from application to application.
Some domains are clear cut in favor of the reliable digital machine model --- for example, due to regulatory issues \citep{dongarra2014applied}.
However, a subset of HPC applications can tolerate --- or even harness --- occasionally flawed or even fundamentally nondeterministic computation \citep{chakradhar2010best}.
Various approximation algorithms or heuristics fall into this category, with notable work being done on best-effort stochastic gradient descent for artificial neural network applications \citep{dean2012large,zhao2019elastic,niu2011hogwild,noel2014dogwild,rhodes2020real}.
Best-effort, real-time computing approaches have also been used in some artificial life models \citep{ray1995proposal}.
Likewise, algorithms relying on pseudo-stochastic methods that tend to exploit noise (rather than destabilize due to it) also make good candidates \citep{chakrapani2008probabilistic,chakradhar2010best}.
Real-time control systems that cannot afford to pause or retry, by necessity, fall into the best-effort category \citep{rahmati2011computing, rhodes2020real}.
\dissertationonly{%
For this dissertation we will, of course, focus on this latter case of systems well-suited to best-effort methods, as evolving systems already require noise to fuel variation.
}

This work distills best-effort communication from the larger issue of best-effort computing, paying it special attention and generally pretermiting the larger issue.
Specifically, we investigate the implications of relaxing synchronization and message delivery requirements.
Under this model, the runtime strives to minimize message latency and loss, but guarantees elimination of neither.
Instead, processes continue their compute work unimpeded and incorporate communication from collaborating processes as it happens to become available.
We still assume that messages, if and when they are delivered, retain contentual integrity.

We see best-effort communication as a particularly fruitful target for investigation.
Firstly, synchronization constitutes the root cause of many contemporary scaling bottlenecks, well below the mark of thousands or millions of cores where runtime failures and soft errors become critical considerations.
Secondly, future HPC hardware is expected to provide more heterogeneous, more variable (i.e., due to power management), and generally lower (relative to compute) communication bandwidth \citep{gropp2013programming, acun2014parallel}; a best-effort approach suits these challenges.
A best-effort communication model presents the possibility of runtime adaptation to effectively utilize available resources given the particular ratio of compute and communication capability at any one moment in any one rack.

Complex biological organisms exhibit characteristic best-effort properties: trillions of cells interact asynchronously while overcoming all but the most extreme failures in a noisy world.
As such, bio-inspired algorithms present strong potential to benefit from best-effort communication strategies.
For example, evolutionary algorithms commonly use guided stochastic methods (i.e., selection and mutation operators) resulting in a search process that does not guarantee optimality, but typically produces a diverse range of high-quality results.
Indeed, island model genetic algorithms are easy to parallelize and have been shown to perform well with asynchronous migration \citep{izzo2009parallel}.
Likewise, artificial life simulations commonly rely on a bottom-up approach and seek to model life-as-it-could-be evolving in a noisy environment akin to the natural world, yet distinct from it \citep{bonabeau1994we}. % rather than the implications of certain mechanistic assumptions
Although perfect reproducibility and observability have uniquely enabled digital evolution experiments to ask and answer otherwise intractable questions \citep{pontes2020evolutionary,lenski2003evolutionary,grabowski2013case,dolson2020interpreting,fortuna2019coevolutionary,goldsby2014evolutionary,covert2013experiments,zaman2011rapid,bundy2021footprint,dolson2017spatial}, the reliable digital machine model is not strictly necessary for all such work.
Issues of distributed and parallel computing are of special interest within the the artificial life subdomain of open-ended evolution (OEE) \citep{ackley2014indefinitely}, which studies long-term dynamics of evolutionary systems in order to understand factors that affect potential to generate ongoing novelty \citep{taylor2016open}.
Recent evidence suggests that the generative potential of at least some model systems are --- at least in part --- meaningfully constrained by available compute resources \citep{channon2019maximum}.

Much exciting work on best-effort computing has incorporated bespoke experimental hardware \citep{chippa2014scalable, ackley2011homeostatic, cho2012ersa, chakrapani2008probabilistic, rhodes2020real}.
However, here, we focus on exploring best-effort communication among parallel and distributed elements within existing, commercially-available hardware.
Existing software libraries, though, do not explicitly expose a convenient best-effort communication interface for such work.
As such, best-effort approaches remain rare in production software and efforts to study best-effort communication must make use of a combination of limited existing support and the development of new software tools.

The Message Passing Interface (MPI) standard \citep{gropp1996high} represents the mainstay for high-performance computing applications.
This standard exposes communication primitives directly to the end user.
MPI's nonblocking communication primitives, in particular, are sufficient to program distributed computations with relaxed synchronization requirements.
Although its explicit, the imperative nature of the MPI protocols enables precise control over execution; unfortunately it also poses significant expense in terms of programmability.
This cost manifests in terms of reduced programmer productivity and software quality, while increasing domain knowledge requirements and the effort required to tune for performance due to program brittleness \citep{gu2019comparative, tang2014mpi}.

In response to programmability concerns, many frameworks have arisen to offer useful parallel and distributed programming abstractions.
Task-based frameworks such as Charm++ \citep{kale1993charm++}, Legion \citep{bauer2012legion}, Cilk \citep{blumofe1996cilk}, and Threading Building Blocks (TBB) \citep{reinders2007intel} describe the dependency relationships among computational tasks and associated data and relies on an associated runtime to automatically schedule and manage execution.
These frameworks assume a deterministic relationship between tasks.
In a similar vein, programming languages and extensions like Unified Parallel C (UPC) \citep{el2006upc} and Chapel \citep{chamberlain2007parallel} rely on programmers to direct execution, but equips them with powerful abstractions, such as global shared memory.
However, Chapel's memory model explicitly forbids data races and UPC ultimately relies on a barrier model for data transfer.

To bridge these shortcomings, we employ a new software framework, the Conduit C++ Library for Best-Effort High Performance Computing \citep{moreno2021conduit}.
The Conduit library provides tools to perform best-effort communication in a flexible, intuitive interface and uniform inter-operation of serial, parallel, and distributed modalities.
Although Conduit currently implements distributed functionality via MPI intrinsics, in future work we will explore lower-level protocols like InfiniBand Unreliable Datagrams \citep{kashyap2006ip, koop2007high}.

Here, we present a set of on-hardware experiments to empirically characterize Conduit's best-effort communication model.
In order to survey across workload profiles, we tested performance under both a communication-intensive graph coloring solver and a compute-intensive artificial life simulation.

First, we determine whether best-effort communication strategies can benefit performance compared to the traditional perfect communication model.
We considered two measures of performance: computational steps executed per unit time and solution quality achieved within a fixed-duration run window.

We compare the best-effort and perfect-computation strategies across processor counts, expecting to see the marginal benefit from best-effort communication increase at higher processor counts.
We focus on weak scaling, growing overall problem size proportional to processor count.
Put another way, we hold problem size per processor constant.%
\footnote{
As opposed to strong scaling, where the problem size is held fixed while processor count increases.
}
This approach prevents interference from shifts in processes' workload profiles in observation of the effects of scaling up processor count.

To survey across hardware configurations, we tested scaling CPU count via threading on a single node and scaling CPU count via multiprocessing with each process assigned to a distinct node.
In addition to a fully best-effort mode and a perfect communication mode, we also tested two intermediate, partially synchronized modes: one where the processor pool completed a global barrier (i.e., they aligned at a synchronization point) at predetermined, rigidly scheduled timepoints and another where global barriers occurred on a rolling basis spaced out by fixed-length delays from the end of the last synchronization.%
\footnote{
Our motivation for these intermediate synchronization modes was interest in the effect of clearing any potentially-unbounded accumulation of message backlogs on laggard processes.
}

Second, we sought to more closely characterize variability in message dispatch, transmission, and delivery under the best-effort model.
Unlike perfect communication, real-time volatility affects the outcome of computation under the best-effort model.
Because real-time processing speed degradations and message latency or loss alters inputs to simulation elements, characterizing the distribution of these phenomena across processing components and over time is critical to understanding the actual computation being performed.
For example, consistently faster execution or lower messaging latency for some subset of processing elements could violate uniformity or symmetry assumptions within a simulation.
It is even possible to imagine reciprocal interactions between real-time best-effort dynamics and simulation state.
In the case of a positive feedback loop, the magnitude of effects might become extreme.
For example, in artificial life scenarios, agents may evolve strategies that selectively increase messaging traffic so as to encumber neighboring processing elements or even cause important messages to be dropped.

We monitor five aspects of real-time behavior, which we refer to as quality of service metrics \citep{karakus2017quality},
\begin{itemize}
  \item wall-time simulation update rate (``simstep period''),
  \item simulation-time message latency,
  \item wall-time message latency,
  \item steadiness of message inflow (``delivery clumpiness''), and
  \item delivery failure rate.
\end{itemize}

In an initial set of experiments, we use the graph coloring problem to test this suite of quality of service metrics across runtime conditions expected to strongly influence them.
We compare
\begin{itemize}
  \item increasing compute workload per simulation update step,
  \item within-node versus between-node process placement, and
  \item multithreading versus multiprocessing.
\end{itemize}
We perform these experiments using a graph coloring solver configured to maximize communication relative to computation (i.e., just one simulation element per CPU) in order to maximize sensitivity of quality of service to the runtime manipulations.

Finally, we extend our understanding of performance scaling from the preceding experiments by analyzing how each quality of service metric fares as problem size and processor count grow together, a ``weak scaling'' experiment.
This analysis would detect a scenario where raw performance remains stable under weak scaling, but quality of service (and, therefore, potentially quality of computation) degrades.
