\section{Introduction}

The parallel and distributed processing capacity of high-performance computing clusters continues to grow rapidly and enable profound scientific and industrial innovations \cite{gagliardi2019international}.
These advances in hardware capacity and economy afford great opportunity, but also pose a serious challenge: developing approaches to effectively harness it.
As HPC systems scale, deterministic algorithms depending on global synchronization become increasingly costly \cite{gropp2013programming,dongarra2014applied}.
Yet, the assumption of deterministic computation and communication remains near-universal within parallel algorithm design \cite{chakradhar2010best}.
In some cases, it suffices to assume hardware anomalies occur negligibly, with any that do transpire likely to cause an (acceptably rare) runtime performance degradation, hang, or crash.
In other cases, hardware anomalies are anticipated in software but handled in a manner guaranteed to preserve perfect algorithmic integrity, like waiting for an anticipated message, requesting a message resend or restarting from a known-good checkpoint.

However, a subset of HPC applications can tolerate fundamentally nondeterministic, asynchronous computation.
Problems with a broad range of acceptable outputs, like optimization or search problems, make good candidates.
Likewise, algorithms relying on pseudo-stochastic methods that tend to iron out (rather than magnify) noise also make good candidates.
The best-effort computing approach proposes that such applications might accept flawed, nondeterministic computation in exchange for improvements to speed and scalability.
Interest in this trade-off motivates the notion of best-effort computing, algorithm design that anticipates hardware anomalies and accepts or even harnesses, rather than corrects, their impacts on computation.

Best-effort approaches have been shown to improve speed \cite{chakrapani2008probabilistic}, energy efficiency \cite{bocquet2018memory}, and scalability \cite{meng2009best}.
Theoretical exploration of constraints distributed systems will face at the asymptote of technological (and even physical) constraints, performed under the banner of ``indefinite scalability'' in \cite{ackley2011pursue}, highlights an essential role --- at large enough scales, essentially a design inevitability --- for best-effort methods.
Specifically, this theory finds as necessary asynchronous operation and graceful degradation under hardware failure (in addition to decentralized networking and interchangeable hardware components).

Bio-inspired algorithms present strong potential to benefit from best-effort communication strategies.
For example, evolutionary algorithms commonly use pseudo-stochastic methods (i.e., selection and mutation operators) and amount to optimization or search with many acceptable results.
Indeed, island model genetic algorithms have been shown to perform well with asynchronous migration \cite{izzo2009parallel}.
Likewise, artificial life simulations commonly rely on a pseudo-stochastic bottom-up approach and seek to model life-as-it-could-be rather than the implications of certain mechanistic assumptions \cite{bonabeau1994we}.
Issues of distributed and parallel computing are of special interest within the the artificial life subdomain of open-ended evolution (OEE) \cite{ackley2014indefinitely}, which studies long-term dynamics of evolutionary systems in order to understand factors that affect potential to generate ongoing novelty \cite{taylor2016open}.
Recent evidence suggests that the generative potential of at least some model systems are --- at least in part --- meaningfully constrained by available compute resources \cite{channon2019maximum}.
Elsewhere, there has also been some interest in best-effort communication stochastic gradient descent, commonly used in conjunction with backpropagation to train artificial neural networks \cite{niu2011hogwild,noel2014dogwild}.

Much exciting work on best-effort computing has incorporated bespoke experimental hardware \cite{chippa2014scalable, ackley2011homeostatic, cho2012ersa, chakrapani2008probabilistic}.
However, here, we focus on exploring best-effort communication among parallel and distributed elements within existing, commercially-available hardware.
Existing software, though, do not explicitly expose a convenient best-effort communication interface.

The Message Passing Interface (MPI) standard \cite{gropp1996high} represents the mainstay for high-performance computing applications.
This standard exposes communication primitives directly to the end user.
MPI's nonblocking communication primitives, in particular, are sufficient to program distributed computations with relaxed synchronization requirements.
Although its explicit, imperative nature enables precise control over execution, it also poses significant expense in terms of programability.
This cost manifests in terms of programmer productivity, domain knowledge requirements, software quality, and difficulty tuning for performance due to program brittleness \cite{gu2019comparative, tang2014mpi}.

In response to programmability concerns, many frameworks have arisen to offer useful parallel and distributed programming abstractions.
Task-based frameworks such as Charm++ \cite{kale1993charm++}, Legion \cite{bauer2012legion}, Cilk \cite{blumofe1996cilk}, and Threading Building Blocks (TBB) \cite{reinders2007intel} describe the dependency relationships among computational tasks and associated data and relies on an associated runtime to automatically schedule and manage execution.
These frameworks assume a deterministic relationship between tasks.
In a similar vein, programming languages and extensions like Unified Parallel C (UPC) \cite{el2006upc} and Chapel \cite{chamberlain2007parallel} rely on programmers to direct execution, but equips them with powerful abstractions, such as global shared memory.
However, Chapel's memory model explicitly forbids data races and UPC ultimately relies on a barrier model for data transfer.

To bridge this software shortcoming, we employ a new software framework, the Conduit C++ Library for Best-Effort High Performance Computing \cite{moreno2021conduit}.
The Conduit library wraps perform best-effort communication in a flexible, intuitive interface that provides uniform inter-operation of serial, parallel, and distributed modalities.
Although Conduit currently implements distributed functionality via MPI intrinsics, in future work we are interested in exploring lower-level protocols like InfiniBand Unreliable Datagrams \cite{kashyap2006ip, koop2007high}.

Here, we present a set of on-hardware experiments to empirically characterize Conduit's best-effort communication model.
First, we determine whether best-effort communication strategies can benefit performance compared to the traditional perfect communication model.
We also tested two intermediate, partially synchronized modes: one where the processor pool completed a global barrier at predetermined, scheduled timepoints and another where global barriers occurred on a rolling basis at fixed intervals.
\footnote{
Our motivation for these intermediate synchronization modes was interest in the effect of clearing any potentially-unbounded accumulation of message backlogs on laggard processes.
}
We test across processor counts, expecting to see benefit from best-effort communication increase at higher processor counts.
We focus on weak scaling, growing overall problem size proportional to processor count.
Put another way, we hold problem size per processor constant.
\footnote{
As opposed to strong scaling, where the problem size is held fixed while processor count increases.
}
This approach prevents interference from shifts in processes' workload profiles in observation of the effects of scaling up processor count.
In order to survey across workload profiles, we tested performance under both a communication-intensive graph coloring solver and a compute-intensive artificial life simulation.
To survey across hardware configurations, we tested scaling CPU count via threading on a single node and scaling via multiprocessing with each process assigned to a distinct node.

Second, we sought to more closely characterize variability in message dispatch, transmission, and delivery under the best-effort model.
Unlike perfect communication, under the best-effort model real-time hardware volatility affects the outcome of computation.
Thus, an understanding of the magnitude and spatiotemporal distribution of variability underpins understanding of the actual computation being performed.
For example, consistently faster execution or lower messaging latency for some subset of processing elements might violate symmetry assumptions within a simulation.
It is even possible to imagine reciprocal interactions between real-time best-effort dynamics and simulation state.
In the case of a positive feedback loop, the magnitude of effects might become extreme.
For example, in artificial life scenarios, agents' evolved strategy might selectively increase messaging traffic to encumber neighboring processing elements or perhaps --- in the case where some fraction of traffic is hostile --- even inducing an increase in overall likelihood of messages being dropped.

We monitor five aspects of real-time behavior, which we refer to as quality of service metrics,
\begin{itemize}
  \item real-time simulation update rate (``simstep period''),
  \item simulation-time message latency,
  \item real-time message latency,
  \item steadiness of message inflow (``delivery clumpiness''), and
  \item delivery failure rate.
\end{itemize}

In an initial set of experiments, we use the graph coloring problem to test this suite of quality of service metrics across runtime conditions expected to strongly influence them.
We compare
\begin{itemize}
  \item increasing compute workload per simulation update step,
  \item within-node versus between-node process placement, and
  \item multithreading versus multiprocessing.
\end{itemize}
We perform these experiments using the graph coloring solver configured to maximize communication relative to computation (i.e., just one simulation element per CPU) in order to maximize sensitivity of quality of service to the runtime manipulations.

Then, to flesh out the understanding of how raw performance scales that was obtained in preceding experiments, we analyze how each quality of service metric fares under weak scaling of process count.
This analysis would detect a scenario where raw performance remains stable under weak scaling, but quality of service (and, therefore, likely quality of computation) degrades.
