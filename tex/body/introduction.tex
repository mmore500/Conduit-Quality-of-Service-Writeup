\section{Introduction}

The parallel and distributed processing capacity of high-performance computing clusters continues to grow rapidly and enable profound scientific and industrial innovations \cite{gagliardi2019international}.
These advances in hardware capacity and economy afford great opportunity, but also pose a serious challenge: developing approaches to effectively harness it.
As HPC systems scale, deterministic algorithms depending on global synchronization become increasingly costly \cite{gropp2013programming,dongarra2014applied}.
Yet, the assumption of deterministic computation and communication remains near-universal within parallel algorithm design \cite{chakradhar2010best}.
In some cases, it suffices to assume hardware anomalies occur negligibly, with any that do transpire likely to cause an (acceptably rare) runtime performance degradation, hang, or crash.
In other cases, hardware anomalies are anticipated in software but handled in a manner guaranteed to preserve perfect algorithmic integrity, like waiting for an anticipated message, requesting a message resend or restarting from a known-good checkpoint.

However, a subset of HPC applications can tolerate fundamentally nondeterministic, asynchronous computation.
Problems with a broad range of acceptable outputs, like optimization or search problems, make good candidates.
Likewise, algorithms relying on pseudo-stochastic methods that tend to iron out (rather than magnify) noise also make good candidates.
The best-effort computing approach proposes that such applications might accept flawed, nondeterministic computation in exchange for improvements to speed and scalability.
Interest in this trade-off motivates the notion of best-effort computing, algorithm design that anticipates hardware anomalies and accepts or even harnesses, rather than corrects, their impacts on computation.

Best-effort approaches have been shown to improve speed \cite{chakrapani2008probabilistic}, energy efficiency \cite{bocquet2018memory}, and scalability \cite{meng2009best}.
Theoretical exploration of constraints distributed systems will face at the asymptote of technological (and even physical) constraints, performed under the banner of ``indefinite scalability'' in \cite{ackley2011pursue}, highlights an essential role --- at large enough scales, essentially a design inevitability --- for best-effort methods.
Specifically, this theory finds as necessary asynchronous operation and graceful degradation under hardware failure (in addition to decentralized networking and interchangeable hardware components).

Bio-inspired algorithms present strong potential to benefit from best-effort communication strategies.
For example, evolutionary algorithms commonly use pseudo-stochastic methods (i.e., selection and mutation operators) and amount to optimization or search with many acceptable results.
Indeed, island model genetic algorithms have been shown to perform well with asynchronous migration \cite{izzo2009parallel}.
Likewise, artificial life simulations commonly rely on a pseudo-stochastic bottom-up approach and seek to model life-as-it-could-be rather than the implications of certain mechanistic assumptions \cite{bonabeau1994we}.
Issues of distributed and parallel computing are of special interest within the the artificial life subdomain of open-ended evolution (OEE) \cite{ackley2014indefinitely}, which studies long-term dynamics of evolutionary systems in order to understand factors that affect potential to generate ongoing novelty \cite{taylor2016open}.
Recent evidence suggests that the generative potential of at least some model systems are --- at least in part --- meaningfully constrained by available compute resources \cite{channon2019maximum}.
Elsewhere, there has also been some interest in best-effort communication stochastic gradient descent, commonly used in conjunction with backpropagation to train artificial neural networks \cite{niu2011hogwild,noel2014dogwild}.

Much exciting work on best-effort computing has incorporated bespoke experimental hardware \cite{chippa2014scalable, ackley2011homeostatic, cho2012ersa, chakrapani2008probabilistic}.
However, here, we focus on exploring best-effort communication among parallel and distributed elements within existing, commercially-available hardware.
Existing software, though, do not explicitly expose a convenient best-effort communication interface.

The Message Passing Interface (MPI) standard \cite{gropp1996high} represents the mainstay for high-performance computing applications.
This standard exposes communication primitives directly to the end user.
MPI's nonblocking communication primitives, in particular, are sufficient to program distributed computations with relaxed synchronization requirements.
Although its explicit, imperative nature enables precise control over execution, it also poses significant expense in terms of programability.
This cost manifests in terms of programmer productivity, domain knowledge requirements, software quality, and difficulty tuning for performance due to program brittleness \cite{gu2019comparative, tang2014mpi}.

In response to programmability concerns, many frameworks have arisen to offer useful parallel and distributed programming abstractions.
Task-based frameworks such as Charm++ \cite{kale1993charm++}, Legion \cite{bauer2012legion}, Cilk \cite{blumofe1996cilk}, and Threading Building Blocks (TBB) \cite{reinders2007intel} describe the dependency relationships among computational tasks and associated data and relies on an associated runtime to automatically schedule and manage execution.
These frameworks assume a deterministic relationship between tasks.
In a similar vein, programming languages and extensions like Unified Parallel C (UPC) \cite{el2006upc} and Chapel \cite{chamberlain2007parallel} rely on programmers to direct execution, but equips them with powerful abstractions, such as global shared memory.
However, Chapel's memory model explicitly forbids data races and UPC ultimately relies on a barrier model for data transfer.

To bridge this software shortcoming, we employ a new software framework, the Conduit C++ Library for Best-Effort High Performance Computing \cite{moreno2021conduit}.
The Conduit library wraps perform best-effort communication in a flexible, intuitive interface that provides uniform inter-operation of serial, parallel, and distributed modalities.
Although Conduit currently implements distributed functionality via MPI intrinsics, in future work we are interested in exploring lower-level protocols like InfiniBand Unreliable Datagrams \cite{kashyap2006ip, koop2007high}.

Here, we present a set of on-hardware experiments to empirically characterize Conduit's best-effort communication model.
First, we determine whether best-effort communication strategies can benefit performance compared to the traditional perfect communication model.
We also tested two intermediate, partially synchronized modes: one where the processor pool completed a global barrier at predetermined, scheduled timepoints and another where global barriers occurred on a rolling basis at fixed intervals.
\footnote{
Our motivation for these intermediate synchronization modes was interest in the effect of clearing any potentially-unbounded accumulation of message backlogs on laggard processes.
}
We test across processor counts, expecting to see benefit from best-effort communication increase at higher processor counts.
We focus on weak scaling, growing overall problem size proportional to processor count.
Put another way, we hold problem size per processor constant.
\footnote{
As opposed to strong scaling, where the problem size is held fixed while processor count increases.
}
This approach prevents interference from shifts in processes' workload profiles in observation of the effects of scaling up processor count.
In order to survey across workload profiles, we tested performance under both a communication-intensive graph coloring solver and a compute-intensive artificial life simulation.
To survey across hardware configurations, we tested scaling CPU count via threading on a single node and scaling via multiprocessing with each process assigned to a distinct node.

Second, we...
We look particularly at how both perfect and best-effort communication approaches fare as problem size increases under weak scaling.

Next, we beyond just raw performance to a more holistic suite of quality of service metrics.
Because, under the best-effort model, hardware volatility affects the outcome of computation in ways that aren't directly controlled or directed by the programmer, it is important to understand the implications of this volatility.

* message latency
* delivery clumpiness
* delivery failure rate
* simstep period

How does variability in these change?
How does mean versus median change?
Is this uniform across time and across (network) space?

experimental
Conduit: In order to understand whether its good and what its implications are, we performed a set of empirical experiments.
We are particularly interested in understanding how it scales --- the idea being that it can not only run well, but maintains stability as hopefully arbitrary numbers of computational elements are added

We begin by trying to understand if there are performance benefits to best-effort communication approach.
* benchmarking experiments: performance under weak scaling

Trying to more closely characterize SOMETHING

Validate and understand these quality of service metrics

We perform these under the most challenging settings of the graph coloring solver --- communication intensive under multiprocessing conditions

to better understand how different runtime conditions affect the an assay of quality of service metrics
* computation versus communication
* intranode versus internode
* multithreading versus multiprocessing

The key
Quality of service metrics under weak scaling
