\subsection{Quality of Service Metrics} \label{sec:quality-of-service-metrics}

\input{fig/quality-of-service-metric-definitions.tex}

The best-effort communication model eschews effort to insulate computation from real-time message delivery dynamics.
Because these dynamics are difficult to predict \textit{a priori} and can bias computation, thorough, empirical runtime measurements are necessary to understand results of such computation.
To this end, we developed a suite of quality of service metrics.
Figure \ref{fig:quality-of-service-metric-definitions} provides space-time diagrams illustrating the metrics presented in this section.

For the purposes of these metrics, we assume that simulations proceed in an iterative fashion with alternating compute and communication phases.
For short, we refer to a single compute-communication cycle as a ``update.''
We derive formulas for metrics in terms of independent observations preceding and succeeding a ``snapshot'' window, during which the simulation and any associated best-effort communication proceeds unimpeded.
Snapshot observations are taken at one minute intervals over the course of each of our a replicate experiments.
The following section, \ref{sec:quality-of-service-experiments}, details the experimental apparatus used to generate quality of service metrics reported in this work.

\subsubsection{Update Period} \label{sec:simstep-period-metric}

We calculate the amount of wall-time elapsed per simulation update cycle (``Update Period'') during a snapshot window as
\begin{align*}
\frac{
  \mathrm{update\,count\,after} - \mathrm{update\,count\,before}
}{
  \mathrm{walltime\,after} - \mathrm{walltime\,before}
}.
\end{align*}
Figure \ref{fig:quality-of-service-metric-definitions-simstep-period} compares a scenario with low update period to a scenario with a higher update period.

\subsubsection{Updates Latency} \label{sec:wall-time-latency-metric}

This metric reports the number of simulation iterations that elapse between message dispatch and message delivery.
Figure \ref{fig:quality-of-service-metric-definitions-latency} compares a scenario with low latency to a scenario with a higher latency.

To prevent interfering effects of imperfect clock synchronization, we estimate one-way wall-time latency from a round-trip measure.
As part of our instrumentation, each simulation unit maintains an independent zero-initialized ``touch counter'' for each neighbor simulation unit it communicates with.
Dispatched messages are bundled with the current value touch counter associated with the target unit.
Every message received sets the corresponding touch counter to $1 + \mathrm{bundled\,touch\,count}$.
In this manner, the touch counter increments by two for each successful round trip completed.
(Because simulation units are arranged as a toroidal mesh in all experiments, interaction between simulation units is always reciprocal.)

We therefore calculate one-way latency during a snapshot window as,
\begin{align*}
  \frac{
    \mathrm{update\,count\,after} - \mathrm{update\,count\,before}
  }{
    \min\Big( \mathrm{ touch\,count\,after } - \mathrm{ touch\,count\,before }, 1 \Big)
  }.
\end{align*}
Note that, in the case where no touches elapse, we bound our latency measure to snapshot duration.

%TODO reference @rodsan's derivation and proof

\subsubsection{Wall-time Latency} \label{sec:simulation-time-latency-metric}

Wall-time latency is closely related to updates latency, simply denominating in elapsed real time instead of elapsed simulation steps.
To calculate wall-time latency we apply a conversion to updates latency based on update period,
\begin{align*}
  \mathrm{update\,latency} \times \mathrm{update\,period}.
\end{align*}

This metric directly tells the real-time performance of message transmission.
Although it directly follows from the interaction between update period and wall-time latency, it complements updates latency's convenient interpretation in terms of potential simulation mechanics (e.g., simulation units tending to see data from two updates ago versus from ten).

In addition to updates latency, Figure \ref{fig:quality-of-service-metric-definitions-latency} is also representative of wall-time latency --- simply interpreting the $y$ axis in terms of wall-time instead of elapsed simulation updates.

\subsubsection{Delivery Failure Rate} \label{sec:delivery-failure-rate-metric}

Delivery failure rate measures the fraction of messages sent that are dropped.
The only condition where messages are dropped is when a send buffer fills.
(Under the existing MPI-based implementation, messages that queue on the send buffer are guaranteed for delivery.)
So, we can calculate
\begin{align*}
  \frac{
    \mathrm{successful\,send\,count\,after} - \mathrm{successful\,send\,count\,before}
  }{
    \mathrm{attempted\,send\,count\,after} - \mathrm{attempted\,send\,count\,before}
  }.
\end{align*}

\subsubsection{Delivery Bunching} \label{sec:delivery-clumpiness-metric}

Delivery bunching quantifies the extent to which message arrival is consolidated to a subset of message pull attempts.
That is, the extent to which independently dispatched messages arrive in batches rather than as an even stream.

If messages all arrive in independent pull attempts, then bunching will be zero.
At the point where the pigeonhole principle applies (i.e., $\mathrm{num\,arriving\,messages} >= \mathrm{num\,pull\,attempts}$), bunching will also be zero so long as every pull attempt is laden.
If all messages arrive during a single pull attempt, then bunching will approach 1.

We formulate bunching as the compliment of steadiness.
(Reporting bunching provides a lower-is-better interpretation consistent with the rest of the quality of service metrics.)
Steadiness, in turn, stems from three component statistics,
\begin{align*}
\mathrm{num\,laden\,pulls\,elapsed} =& \mathrm{laden\,pull\,count\,after} \\
  &- \mathrm{laden\,pull\,count\,before} \\
\mathrm{num\,messages\,received} =& \mathrm{message\,count\,after} \\
  &- \mathrm{message\,count\,before} \\
\mathrm{num\,pulls\,attempted} =& \mathrm{pull\,attempt\,count\,after} \\
  &- \mathrm{pull\,attempt\,count\,before}
.
\end{align*}

Here, we refer to pull attempts that successfully retrieve a message as ``laden.''

We combine $\mathrm{num\,messages\,received}$ and $\mathrm{num\,pulls\,attempted}$ to derive,
\begin{align*}
  \mathrm{num\,opportunities\,for\,laden\,pulls} = \\
   \min\Big(\mathrm{num\,messages\,received}, \mathrm{num\,pulls\,attempted}\Big).
\end{align*}

Then, to calculate steadiness,
\begin{align*}
  \frac{
    \mathrm{num\,laden\,pulls\,elapsed}
  }{
    \mathrm{num\,opportunities\,for\,laden\,pulls}
  }.
\end{align*}

Delivery bunching follows as $1 - \mathrm{steadiness}$.
Figure \ref{fig:quality-of-service-metric-definitions-clumpiness} compares a scenario with low bunching to a scenario with higher bunching.
