\subsection{Quality of Service: Multithreading vs. Multiprocessing}
\label{sec:multithreading-vs-multiprocessing}

This section compares best-effort quality of service under multithreading and multiprocessing schemes.
We hold hardware configuration constant by restricting multiprocessing to cores a single hardware node, as is the case for multithreading.
However, inter-process communication occurred via MPI calls while inter-thread communication occurring via shared memory access mediated by a C++ \texttt{std::mutex}.

The graph coloring benchmark again serves as our experimental model.
Both treatments used a single pair of CPUs.

\subsubsection{Simstep Period}

Multithreading enabled faster simulation update turnover than multiprocessing.

% Under multithreading, simstep period was around \SI{5}{\micro\second} (mean: \SI{4.62}{\micro\second} inlet / \SI{4.60}{\micro\second} outlet; median: \SI{4.64}{\micro\second} inlet / outlet).
Under multithreading, simstep period was around \SI{5}{\micro\second} (mean: \SI{4.60}{\micro\second}; median: \SI{4.64}{\micro\second}).
% Simstep period for multiprocessing was around \SI{9}{\micro\second} (mean: \SI{9.01}{\micro\second} inlet / \SI{8.98}{\micro\second} outlet; median: \SI{9.07}{\micro\second} inlet / \SI{9.01}{\micro\second} outlet).
Simstep period for multiprocessing was around \SI{9}{\micro\second} (mean: \SI{9.00}{\micro\second}; median: \SI{9.04}{\micro\second}).
Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-distribution-simstep-period-inlet-ns} and \ref{fig:multithreading-vs-multiprocessing-distribution-simstep-period-outlet-ns} depict the distribution of walltime per computational update for both multiprocessing and multithreading.
This result falls in line with expectations that interaction via shared memory incurs lower overhead than via MPI calls.

Regression analysis showed that both mean and median simstep period were significantly slower under multiprocessing compared to multithreading.
(Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-regression-simstep-period-inlet-ns} and \ref{fig:multithreading-vs-multiprocessing-regression-simstep-period-outlet-ns} visualize these regressions and Supplementary Tables \ref{tab:multithreading-vs-multiprocessing-ordinary-least-squares-regression} and \ref{tab:multithreading-vs-multiprocessing-quantile-regression} detail numerical results.)

\subsubsection{Walltime Latency}

No significant difference in walltime latency was detected between multiprocessing and multithreading.

% In the median case, walltime latency was approximately \SI{5}{\micro\second} for multithreading (\SI{5.05}{\micro\second} inlet / \SI{5.08}{\micro\second} outlet) and \SI{8}{\micro\second} for multiprocessing (\SI{7.84}{\micro\second} inlet / \SI{7.80}{\micro\second} outlet).
In the median case, walltime latency was approximately \SI{5}{\micro\second} for multithreading and \SI{8}{\micro\second} for multiprocessing.
% However, a pair of extreme outliers among snapshot windows --- with walltime latencies of approximately 12ms --- drove multithreading walltime latency much higher in the mean case (\SI{448}{\micro\second} inlet / \SI{454}{\micro\second} outlet).
However, a pair of extreme outliers among snapshot windows --- with walltime latencies of approximately 12ms --- drove multithreading walltime latency much higher in the mean case (\SI{451}{\micro\second}).
% In the mean case, multiprocessing walltime latency was \SI{8.65}{\micro\second} inlet / \SI{8.48}{\micro\second} outlet.
In the median case, multiprocessing walltime latency was \SI{8.56}{\micro\second}.

Cache invalidation or mutex contention provide possible explanations for the observed episodes of extreme multithreading latency, although magnitude on the order of milliseconds for such effects is surprising.
Multithreading appears to provide marginally lower latency service in the median case, but at the cost of vulnerability to extreme high-latency disruptions.

Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-distribution-latency-walltime-inlet-ns} and \ref{fig:multithreading-vs-multiprocessing-distribution-latency-walltime-outlet-ns} show the distributions of walltime latency for multithread and multiprocess runs.
Regression analysis did not detect any significant difference in walltime latency between multithreading and multiprocessing (Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-regression-latency-walltime-inlet-ns}, \ref{fig:multithreading-vs-multiprocessing-regression-latency-walltime-outlet-ns}; Supplementary Tables \ref{tab:multithreading-vs-multiprocessing-ordinary-least-squares-regression} and \ref{tab:multithreading-vs-multiprocessing-quantile-regression}).

\subsubsection{Simstep Latency}

No significant difference in simstep latency was detected between multiprocessing and multithreading.

In the median case, multiprocessing offered marginally lower simstep latency than multithreading.
% Median simstep latency was 0.84 updates inlet/outlet under multiprocessing and 1.10 updates inlet / 1.11 updates outlet under multithreading.
Median simstep latency was 0.84 updates under multiprocessing and 1.10 updates under multithreading.
However, just as for walltime latency, extreme magintude outliers ($\approx$ \numprint{2000} simsteps) boosted mean simstep latency for multithreading.
% Mean simstep latency was 0.95 updates inlet / 0.94 updates outlet under multiprocessing and 77.0 updates inlet / 78.0 updates outlet under multithreading.
Mean simstep latency was 0.94 updates under multiprocessing and 78.0 updates under multithreading.
Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-distribution-latency-simsteps-inlet} and \ref{fig:multithreading-vs-multiprocessing-distribution-latency-simsteps-outlet} compare the distributions of simstep latency across these conditions.

Direct measurements of simstep period and walltime latency suggest that faster simstep period, rather than slower walltime latency, explain the marginally higher simstep latency under multithreading.

Regression analysis detected no significant effect of threading versus processing on simstep latency in both the mean and median cases (Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-regression-latency-simsteps-inlet} and \ref{fig:multithreading-vs-multiprocessing-regression-latency-simsteps-outlet}).
Supplementary Tables \ref{tab:multithreading-vs-multiprocessing-ordinary-least-squares-regression} and \ref{tab:multithreading-vs-multiprocessing-quantile-regression} detail numerical results of these regressions.

\subsubsection{Delivery Clumpiness}

Multithreading exhibited higher median clumpiness and greater variance in clumpiness than multiprocessing.

Under multithreading, clumpiness was nearly 1 within some snapshot windows and less than 0.1 within others.
Under multiprocessing, clumpiness was consistently less than 0.1.
Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-distribution-delivery-clumpiness} and \ref{fig:multithreading-vs-multiprocessing-distribution-delivery-clumpiness} show the distributions of clumpiness under both multiprocessing and multithreading.
Multithreading median clumpiness was 0.54.
Multiprocessing median clumpiness was 0.03.
Multithreading and multiprocessing mean clumpinesses were 0.56 and 0.03, respectively.

Regression analysis confirmed a significantly greater clumpiness under both multithreading compared to multiprocessing (Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-regression-delivery-clumpiness}, \ref{fig:multithreading-vs-multiprocessing-regression-delivery-clumpiness}; Supplementary Tables \ref{tab:multithreading-vs-multiprocessing-ordinary-least-squares-regression} and \ref{tab:multithreading-vs-multiprocessing-quantile-regression}).
% This result falls in line with other results suggesting that multithreading providing less consistent quality of service than multiprocessing.

\subsubsection{Delivery Failure Rate}

We observed a higher proportion of deliveries fail for multiprocessing than for multithreading.
(This is as expected; the multithread implementation directly wrote updates to a piece of shared memory, so there was no send buffer to backlog and induce message drops.)

Multiprocessing exhibited both mean and median delivery failure rate of 0.38.
In individual multiprocessing snapshot windows, we observed a delivery failure rates ranging from less than 0.1 to as high as 0.7.
We observed no multithreaded delivery failures.
Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-distribution-delivery-clumpiness} and \ref{fig:multithreading-vs-multiprocessing-distribution-delivery-clumpiness} show the distributions of delivery failure rate for multithreading and multiprocessing.

Regression analysis confirmed a significant increase in delivery failure under multiprocessing (Supplementary Figures \ref{fig:multithreading-vs-multiprocessing-regression-delivery-clumpiness}, \ref{fig:multithreading-vs-multiprocessing-regression-delivery-clumpiness}; Supplementary Tables \ref{tab:multithreading-vs-multiprocessing-ordinary-least-squares-regression} and \ref{tab:multithreading-vs-multiprocessing-quantile-regression}).
