\subsection{Quality of Service: Computation vs. Communication}
\label{sec:computation-vs-communication}

Having shown performance benefits of best-effort communication on the graph coloring and digital evolution benchmarks in Sections \ref{sec:multithread-benchmarks} and \ref{sec:multiprocess-benchmarks}, we next seek to more fully characterize the best-effort approach using a holistic suite of proposed quality of service metrics.

This section evaluates how a simulation's ratio of communication intensity to computational work affects these quality of service metrics.
The graph coloring benchmark serves as our experimental model.

For this experiment, arbitrary compute work (detached from the underlying algorithm) was added to the simulation update process.
We used a call to the \texttt{std::mt19937} random number engine as a unit of compute work.
In microbenchmarks, we found that one work unit consumed about 35ns of walltime and 21ns of compute time.
We performed 5 treatments, adding \numprint{0}, \numprint{64}, \numprint{4096}, \numprint{262144}, or \numprint{16777216} units of compute work to the update process.
For each treatment, measurements were made on a pair of processes split across different nodes.

\subsubsection{Simstep Period}

Unsurprisingly, we found a direct relationship between per-update computational workload and the walltime required per computational update.

Supplementary Figures \ref{fig:computation-vs-communication-distribution-simstep-period-inlet-ns} and \ref{fig:computation-vs-communication-distribution-simstep-period-outlet-ns} depict the distribution of walltime per computational update across snapshots.
Once added compute work supersedes the light compute work already associated with the graph coloring algorithm update step (at around 64 work units), simstep period scales in direct proportion with compute work.

Indeed, we found a significant positive relationship between both mean and median simstep period and added compute work (Supplementary Figures \ref{fig:computation-vs-communication-regression-simstep-period-inlet-ns} and \ref{fig:computation-vs-communication-regression-simstep-period-outlet-ns}).
% At 0 units of added compute work, mean and median simstep period was 14.7 \SI{14.7}{\micro\second} (both inlet/outlet).
At 0 units of added compute work, mean and median simstep period was 14.7 \SI{14.7}{\micro\second}.
% At \numprint{16777216} units of added compute work, mean simstep period was 614ms inlet/608ms outlet and median simstep period was 506ms inlet/508ms outlet.
At \numprint{16777216} units of added compute work, mean simstep period was 611ms and median simstep period was 507ms.
Supplementary Tables \ref{tab:computation-vs-communication-ordinary-least-squares-regression} and \ref{tab:computation-vs-communication-quantile-regression} detail numerical results of these regressions.

\subsubsection{Simstep Latency}

Unsurprisingly, again, we observed a negative relationship between the number of simulation steps elapsed during message transit and added computational work.
Put simply, longer update steps provide more time for messages to transit.

Supplementary Figures \ref{fig:computation-vs-communication-distribution-latency-simsteps-inlet} and \ref{fig:computation-vs-communication-distribution-latency-simsteps-outlet} show the distribution of simstep latency across compute workloads.
% With no added compute work, messages take between 20 and 100 simulation steps to transit (mean: 48.2 updates inlet/47.9 updates outlet; median: 42.5 updates inlet/outlet).
With no added compute work, messages take between 20 and 100 simulation steps to transit (mean: 48.0 updates; median: 42.5 updates).
At maximum compute work per update, messages arrive at a median 1.00 update latency.

Regression analysis confirms a significant negative relationship between both mean and median log simstep latency and log added compute work (Supplementary Figures \ref{fig:computation-vs-communication-regression-latency-simsteps-inlet} and \ref{fig:computation-vs-communication-regression-latency-simsteps-outlet}).
Supplementary Tables \ref{tab:computation-vs-communication-ordinary-least-squares-regression} and \ref{tab:computation-vs-communication-quantile-regression} detail numerical results of these regressions.

%TODO use log latency simsteps for estimated statistics --- regressions are broken for raw input

\subsubsection{Walltime Latency}

Effects of compute work on walltime latency highlight an important caveat in interpretation of this metric.

% At \numprint{0}, \numprint{64}, and \numprint{4096} work units, walltime latency measures $\approx 1$ ms (means: \SI{710}{\micro\second}, \SI{794}{\micro\second}, \SI{906}{\micro\second} inlet / \SI{706}{\micro\second}, \SI{782}{\micro\second}, \SI{899}{\micro\second}; medians: \SI{623}{\micro\second}, \SI{639}{\micro\second}, \SI{742}{\micro\second} inlet / \SI{622}{\micro\second}, \SI{642}{\micro\second}, \SI{733}{\micro\second} outlet).
At \numprint{0}, \numprint{64}, and \numprint{4096} work units, walltime latency measures $\approx 1$ ms (means: \SI{708}{\micro\second}, \SI{788}{\micro\second}, \SI{902}{\micro\second}; medians: \SI{622}{\micro\second}, \SI{640}{\micro\second}, \SI{738}{\micro\second}).
However, once simstep period grows to $\approx 10$ ms at \numprint{262144} work units and (an order of magnitude in excess of walltime latency observed at low compute loads), walltime latency increases with added compute work.
% At \numprint{16777216} compute work units, 1.00s inlet / 1.01s outlet median walltime latency is observed.
At \numprint{16777216} compute work units, 1.00s median walltime latency is observed.

Because our computational model assumes on-demand message delivery with a communication phase only occurring once per simulation update, message transmission speed is fundamentally limited by simulation update period.
If a message is dispatched while its recipient is busy doing computational work, the soonest it can be received will be when that recipient completes the computational phase of its update.
In order to measure transmission time fully independent of delays due to on-demand delivery, additional instrumentation would be necessary.
However, when this latency is greater than a few simsteps, this measure is reasonably representative of message transmission time.

Supplementary Figures \ref{fig:computation-vs-communication-distribution-latency-walltime-inlet-ns} and \ref{fig:computation-vs-communication-distribution-latency-walltime-outlet-ns} show the distribution of walltime latency across computational workloads.
Supplementary Figures \ref{fig:computation-vs-communication-regression-latency-walltime-inlet-ns} and \ref{fig:computation-vs-communication-regression-latency-walltime-outlet-ns} summarize regression between walltime latency and added compute work.
Supplementary Tables \ref{tab:computation-vs-communication-ordinary-least-squares-regression} and \ref{tab:computation-vs-communication-quantile-regression} detail numerical results of those regressions.

\subsubsection{Delivery Clumpiness}

We observed a significant negative relationship between computation workload and delivery clumpiness.

At low computational intensity, we observed clumpiness greater than 0.95, meaning that fewer than 5\% of pull requests were laden with fresh messages (at 0 compute work mean: 0.96, median 0.96).
However, at high computational intensity clumpiness reached 0, indicating that messages arrived as a steady stream (at \numprint{16777216} compute work mean: 0.00, median 0.00).
Presumably, the reduction in clumpiness is due to increased real-time separation between dispatched messages.

Supplementary Figure \ref{fig:computation-vs-communication-distribution-delivery-clumpiness} shows the effect of computational workload on the distribution of observed clumpinesses.
We found a significant negative relationship between both mean and median clumpiness and computational intensity.
Supplementary Figure \ref{fig:computation-vs-communication-regression-delivery-clumpiness} visualizes these regressions and Supplementary Tables \ref{tab:computation-vs-communication-ordinary-least-squares-regression} and \ref{tab:computation-vs-communication-quantile-regression} provide numerical details.

\subsubsection{Delivery Failure Rate}

We did not observe any delivery failures across all replicates and all compute workloads.
So, compute workload had no observable effect on delivery reliability.
% Interestingly, as discussed in Section \ref{sec:intranode-vs-internode}, we did observe some delivery failure under intranode conditions.
% However, these experiments were conducted under internode conditions.

Supplementary Figure \ref{fig:computation-vs-communication-distribution-delivery-failure-rate} shows the distribution of delivery failure rates across computation workloads and Supplementary Figure \ref{fig:computation-vs-communication-regression-delivery-failure-rate} shows regressions of delivery failure rate against computational workload.
See Supplementary Tables \ref{tab:computation-vs-communication-ordinary-least-squares-regression} and \ref{tab:computation-vs-communication-quantile-regression} for numerical details.
