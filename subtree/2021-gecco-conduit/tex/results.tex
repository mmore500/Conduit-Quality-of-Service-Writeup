\section{Results and Discussion}

\subsection{Multithread Benchmarks}

\input{fig/multithread_benchmarks.tex}

Figure \ref{fig:multithread_graph_coloring_update_rate} presents per-cpu algorithm update rate for the graph coloring benchmark at 1, 4, 16, and 64 threads.
Update rate performance decreased with increasing multithreading across all asynchronicity modes.
This performance degradation was rather severe --- per-cpu update rate decreased by 61\% between 1 and 4 threads and by about another 75\% between 4 and 64 threads.
Surprisingly, this issue appears largely unrelated to inter-thread communication, as it was also observed in asynchronicity mode 4, where all interthread communication is disabled.
Perhaps per-cpu update rate degradation under threading was induced by strain on a limited system resource like memory cache or access to the system clock (which was used to control run timing).
This unexpectedly severe phenomenon merits further investigation to fully in future work with this benchmark.

Nevertheless, we were able to observe significantly better performance of best-effort asynchronicity modes 1, 2, and 3 at high thread counts. 
At 64 threads, these run modes significantly outperformed the fully-synchronized mode 0 ($p < 0.05$, non-overlapping 95\% confidence intervals).
Likewise, as shown in Figure \ref{fig:multithread_graph_coloring_solution_quality}, best-effort asynchronicity modes were able to deliver significantly better graph coloring solutions within the allotted compute time than the fully-synchronized mode 0 ($p < 0.05$, non-overlapping 95\% confidence intervals).

Figure \ref{fig:multithread_digital_evolution_update_rate} shows per-cpu algorithm update rate for the digital evolution benchmark at 1, 4, 16, and 64 threads.
Similarly to the graph coloring benchmark, update rate performance decreased with increasing multithreading across all asynchronicity modes --- including mode 4, which eschews inter-thread communication.
Even without communication between threads, with 64 threads each thread performed updates at only 61\% the rate of a lone thread.
At 64 threads, best-effort asynchronicity modes 1, 2, and 3 exhibit about 43\% the update-rate performance of a lone thread.
Although best-effort inter-thread communication only exhibits half the update-rate performance of completely decoupled execution at 64 threads, this update-rate performance is roughly $2.1\times$ that of the fully-synchronous mode 0.
Indeed, best-effort modes significantly outperform the fully-synchronous mode on the digital evolution benchmark at both 16 and 64 threads ($p < 0.05$, non-overlapping 95\% confidence intervals).

\subsection{Multiprocess Benchmarks}

\input{fig/multiprocess_benchmarks.tex}

Figure \ref{fig:multiprocess_graph_coloring_update_rate} shows per-cpu algorithm update rate for the graph coloring benchmark at 1, 4, 16, and 64 processes.
Unlike the multithreaded benchmark, multiprocess graph coloring exhibits consistent update-rate performance across process counts under asynchronicity mode 4, where inter-thread communication is disabled.
This matches the expectation that, indeed, with comparable hardware a single process should exhibit the same mean performance as any number of completely decoupled processes.
At 64 processes, best-effort asynchronicity mode 3 exhibits about 63\% the update-rate performance of single-process execution. 
This represents about an $7.8\times$ speedup compared to fully-synchronous mode 0.
Indeed, best-effort mode 3 enables significantly better per-cpu update rates at 4, 16, and 64 processes ($p < 0.05$, non-overlapping 95\% confidence intervals).

Likewise, shown in Figure \ref{fig:multiprocess_graph_coloring_solution_quality}, best-effort asynchronicity mode 3 yields significantly better graph-coloring results within the allotted time at 4, 16, and 64 processes ($p < 0.05$, non-overlapping 95\% confidence intervals).
Interestingly, partial-synchronization modes 1 and 2 exhibited highly inconsistent solution quality performance at 16 and 64 process count benchmarks.
Fixed-timepoint barrier sync (mode 2) had particularly poor performance performance at 64 processes (note the log-scale axis).
We suspect this was caused by a race condition where workers would assign sync points to different fixed points different based on slightly different startup times (i.e., process 0 syncs at seconds 0, 1, 2... while process 1 syncs at seconds 1, 2, 3..).

Figure \ref{fig:multiprocess_digital_evolution_update_rate} presents per-cpu algorithm update rate for the digital evolution benchmark at 1, 4, 16, and 64 processes.
Relative performance fares well at high process counts under this relatively computation-heavy workload, with 64-process fully best-effort simulation exhibiting about 92\% the update rate of single-process simulation.
This represents a $2.1\times$ speedup compared to the fully-synchronous run mode 0.
Best-effort significantly mode 3 outperforms the per-cpu update rate of fully-synchronous mode 0 at process counts 16 and 64 ($p < 0.05$, non-overlapping 95\% confidence intervals).
